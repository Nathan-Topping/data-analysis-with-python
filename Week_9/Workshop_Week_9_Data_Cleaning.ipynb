{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9: Data Cleaning & Preparation\n",
    "\n",
    "Welcome to one of the most important and time-consuming aspects of data analysis! This week we'll learn how to transform messy, real-world data into a clean, analysis-ready format.\n",
    "\n",
    "## Key Topics:\n",
    "- **Conceptual Framework**: Cleaning for Descriptive vs. Predictive analysis\n",
    "- **Missing Value Imputation**: Strategies for handling missing data\n",
    "- **Outlier Handling**: Identifying and treating extreme values\n",
    "- **Categorical Encoding**: Converting text data into a format for analysis\n",
    "- **Distributional Transformations**: Dealing with skewed data\n",
    "\n",
    "These skills are essential for ensuring the accuracy and reliability of any analysis you perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Mission: Analyse Employee Performance at TechCorp\n",
    "\n",
    "You're a Data Analyst at **TechCorp**. The HR department wants to understand factors driving employee promotion and performance, but their data is messy and comes from multiple sources.\n",
    "\n",
    "**Your Challenge:**\n",
    "- The dataset contains common data quality issues:\n",
    "- **Missing information** for some employees\n",
    "- **Inconsistent text entries** and typos\n",
    "- **Extreme outliers** that could skew results\n",
    "- **Skewed data distributions**\n",
    "\n",
    "**Your Goal:**\n",
    "- Prepare two versions of the dataset:\n",
    "  1. A version for **descriptive analysis** (e.g., for an HR dashboard)\n",
    "  2. A version for **predictive modeling** (e.g., to predict future promotions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Concept: Descriptive vs. Predictive Cleaning\n",
    "\n",
    "This is the most important concept of this workshop. The way you clean your data depends entirely on your goal.\n",
    "\n",
    "### Cleaning for Descriptive Analytics\n",
    "- **Goal**: To present the data as accurately and honestly as possible. We want to understand the data's current state, including its flaws.\n",
    "- **Approach**: Gentle and transparent. We fix obvious errors (like typos) but we don't fundamentally change the data's shape or values. We might report on missing values rather than filling them in.\n",
    "- **Example**: An HR dashboard showing the current distribution of salaries, including outliers, and explicitly stating how many employees have missing performance ratings.\n",
    "\n",
    "### Cleaning for Predictive Modeling\n",
    "- **Goal**: To prepare the data for a machine learning algorithm. Algorithms have strict requirements (e.g., no missing values, all numerical input).\n",
    "- **Approach**: More aggressive and transformative. We fill missing values, convert all text to numbers, and may even alter distributions to help the model learn better.\n",
    "- **Example**: A dataset where all missing values are filled, all categories are converted to numbers, and salary is log-transformed to reduce the impact of outliers on a promotion prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('employee_performance_data.csv')\n",
    "\n",
    "print('Dataset loaded successfully!')\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a quick overview of the data types and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial info\n",
    "df_raw.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for a statistical summary of the numerical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get statistical summary\n",
    "df_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Observations:**\n",
    "- **Missing Values**: `education`, `age`, and `previous_year_rating` have nulls.\n",
    "- **Outliers**: The `max` salary (around 337k) seems much higher than the 75th percentile (around 83k). `tenure_months` also has a high max value.\n",
    "- **Categorical Data**: `department`, `region`, `education`, `gender`, and `recruitment_channel` will need to be handled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cleaning for Descriptive Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal here is to create a clean dataset for an HR dashboard. We want to fix errors but preserve the original story of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for descriptive cleaning\n",
    "df_desc = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Handling Categorical Messiness (Typos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the `department` column for inconsistent values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique values in department\n",
    "df_desc['department'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see 'Enginering' and 'ENGINEERING'. Let's consolidate these into 'Engineering'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidate department names\n",
    "department_map = {\n",
    "    'Enginering': 'Engineering',\n",
    "    'ENGINEERING': 'Engineering'\n",
    "}\n",
    "df_desc['department'] = df_desc['department'].replace(department_map)\n",
    "\n",
    "print('Cleaned department values:')\n",
    "df_desc['department'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Handling Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For descriptive analysis, we often replace missing categorical values with a placeholder like 'Unknown' to be explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing education with 'Unknown'\n",
    "df_desc['education'] = df_desc['education'].fillna('Unknown')\n",
    "\n",
    "print('Value counts for education after filling missing values:')\n",
    "df_desc['education'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical columns, we'll leave them as `NaN`. This is because pandas calculations like `.mean()` or `.sum()` will correctly ignore them by default, which is what we want for honest reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that missing values still exist in numerical columns\n",
    "print(f\"Missing values in 'age': {df_desc['age'].isnull().sum()}\")\n",
    "print(f\"Missing values in 'previous_year_rating': {df_desc['previous_year_rating'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For descriptive analysis, we don't remove outliers. They are part of the story! An HR manager would want to know who has an unusually high salary. Our job is to identify and report on them, not hide them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify salary outliers for reporting\n",
    "salary_q99 = df_desc['salary'].quantile(0.99)\n",
    "outliers = df_desc[df_desc['salary'] > salary_q99]\n",
    "\n",
    "print(f'There are {len(outliers)} employees with salaries above the 99th percentile (${salary_q99:,.2f}).')\n",
    "outliers[['employee_id', 'department', 'salary']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Binning Numeric Data for Descriptive Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to group continuous numbers into categories (or 'bins') for reporting. Let's create age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bins and labels\n",
    "age_bins = [20, 30, 40, 50, 60, 70]\n",
    "age_labels = ['20-29', '30-39', '40-49', '50-59', '60+']\n",
    "\n",
    "df_desc['age_group'] = pd.cut(df_desc['age'], bins=age_bins, labels=age_labels, right=False)\n",
    "\n",
    "print('Age group distribution:')\n",
    "df_desc['age_group'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Descriptive Cleaning\n",
    "\n",
    "Our `df_desc` DataFrame is now ready for reporting. We've:\n",
    "- Corrected typos in categorical data.\n",
    "- Explicitly labeled missing categorical data as 'Unknown'.\n",
    "- Kept numerical missing values and outliers to ensure our reports reflect the true state of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cleaning for Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal now is to prepare the data for a hypothetical model to predict `is_promoted`. This requires more aggressive transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for predictive cleaning\n",
    "df_pred = df_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Handling Missing Values (Imputation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models can't handle missing values. We need to fill them using a reasonable strategy (imputation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical columns, we can use the median. The median is generally safer than the mean when we have skewed data or outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute numerical columns with the median\n",
    "for col in ['age', 'previous_year_rating']:\n",
    "    median_val = df_pred[col].median()\n",
    "    df_pred[col] = df_pred[col].fillna(median_val)\n",
    "    print(f\"Filled missing '{col}' with median value: {median_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For categorical columns, we can use the mode (the most frequent value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute categorical column with the mode\n",
    "mode_val = df_pred['education'].mode()[0]\n",
    "df_pred['education'] = df_pred['education'].fillna(mode_val)\n",
    "\n",
    "print(f\"Filled missing 'education' with mode value: '{mode_val}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Outlier Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme outliers can negatively impact some models. A common strategy is 'capping' or 'winsorizing', where we cap the values at a certain percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cap salary at the 99th percentile\n",
    "q99_salary = df_pred['salary'].quantile(0.99)\n",
    "df_pred['salary'] = df_pred['salary'].clip(upper=q99_salary)\n",
    "\n",
    "print(f'Salaries are now capped at the 99th percentile: ${q99_salary:,.2f}')\n",
    "df_pred.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Distributional Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features with highly skewed distributions (like salary) can sometimes be improved with a log transformation. This can help some models perform better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply a log transformation to salary\n",
    "df_pred['salary_log'] = np.log1p(df_pred['salary'])\n",
    "\n",
    "print('Salary distribution before and after log transform:')\n",
    "df_pred[['salary', 'salary_log']].hist(bins=30, figsize=(10, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models need numerical input. We must convert our categorical columns into numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with a clear order (like `education`), we can map them to numbers manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually encode education level\n",
    "education_map = {\n",
    "    'High School': 1,\n",
    "    \"Bachelor's\": 2,\n",
    "    \"Master's\": 3,\n",
    "    'PhD': 4\n",
    "}\n",
    "df_pred['education_encoded'] = df_pred['education'].map(education_map)\n",
    "\n",
    "df_pred[['education', 'education_encoded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns with no inherent order (like `department`), we use one-hot encoding. This creates a new binary (0/1) column for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode department\n",
    "department_dummies = pd.get_dummies(df_pred['department'], prefix='dept')\n",
    "\n",
    "# Join the new dummy variables back to our dataframe\n",
    "df_pred = pd.concat([df_pred, department_dummies], axis=1)\n",
    "\n",
    "print('DataFrame with one-hot encoded departments:')\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Predictive Cleaning\n",
    "\n",
    "Our `df_pred` DataFrame is now almost ready for a model. We have:\n",
    "- Filled all missing values.\n",
    "- Capped extreme outliers.\n",
    "- Transformed skewed data.\n",
    "- Converted categorical features into numerical formats.\n",
    "\n",
    "The final step would be to drop the original non-numeric columns before feeding the data to a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns AND boolean columns (from one-hot encoding)\n",
    "df_model_ready = df_pred.select_dtypes(include=[np.number, bool]).drop(columns=['is_promoted'])\n",
    "\n",
    "print('Final model-ready dataset shape:')\n",
    "print(df_model_ready.shape)\n",
    "df_model_ready.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: Two Goals, Two Different Datasets\n",
    "\n",
    "Let's compare our two final datasets.\n",
    "\n",
    "**`df_desc` (for Descriptive Analytics):**\n",
    "- Still contains text and missing values (NaNs).\n",
    "- Preserves original data distributions, including outliers.\n",
    "- Perfect for creating honest, transparent business reports and dashboards.\n",
    "\n",
    "**`df_model_ready` (for Predictive Modeling):**\n",
    "- Is fully numerical with no missing values.\n",
    "- Has outliers and distributions transformed to suit algorithms.\n",
    "- Ready to be used to train a machine learning model.\n",
    "\n",
    "Understanding this distinction is a key skill for any data analyst. The right cleaning strategy always depends on your end goal!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
