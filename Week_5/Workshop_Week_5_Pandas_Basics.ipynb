{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop Week 5: Introduction to Pandas\n",
    "\n",
    "This week, we are starting to explore **pandas**, the most popular Python library for data analysis. Pandas provides powerful, easy-to-use data structures and data analysis tools that are essential for any data analyst."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Pandas?\n",
    "\n",
    "Pandas is built on top of another library called **NumPy**. We will look at NumPy in more detail later in the course, but for now, all you need to know is that NumPy provides the high-performance arrays that pandas uses to store data. This makes pandas incredibly fast and efficient, even with large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The DataFrame: Your Data's Home\n",
    "\n",
    "The most important data structure in pandas is the **DataFrame**. A DataFrame is a two-dimensional table of data with rows and columns, much like a spreadsheet. Here are the key organisational principles:\n",
    "\n",
    "- **Rows represent observations:** Each row is a single data record. For example, in a dataset of students, each row would represent one student.\n",
    "- **Columns represent variables/features:** Each column is a specific piece of information about the observations. For our student dataset, columns might include `name`, `age`, and `grade`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Types in a DataFrame\n",
    "\n",
    "A critical rule in pandas is that **each column must have a single data type**. For example, a column for `age` will only contain numbers, and a column for `name` will only contain text. This is important for two reasons:\n",
    "\n",
    "1. **Data Consistency:** It ensures that all the data in a column is of the same type, which prevents errors in your analysis.\n",
    "2. **Performance:** It allows pandas to use efficient, specialised operations for each data type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Real-World Dataset\n",
    "\n",
    "Let's get our hands dirty with a real dataset. We'll be using the **Breast Cancer Wisconsin (Diagnostic) dataset**, which contains information about breast cancer tumors. This is a classic dataset for data analysis and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the artificial dataset we created\n",
    "df = pd.read_csv(\"employee_data.csv\")\n",
    "\n",
    "# Display the entire DataFrame to see our data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes vs Methods in pandas\n",
    "\n",
    "In pandas, **DataFrames** have *attributes* and *methods*.  \n",
    "Both help you understand or work with your data, but they behave differently:\n",
    "\n",
    "- **Attributes** describe something *about* the DataFrame — for example, its size or column names.  \n",
    "  You access them **without parentheses** because they simply *store information*.\n",
    "\n",
    "  `df.shape`     → (number of rows, number of columns)  \n",
    "  `df.columns`   → column names  \n",
    "  `df.dtypes`    → data type of each column  \n",
    "\n",
    "- **Methods** perform an *action* or *operation* on the DataFrame.  \n",
    "  You call them **with parentheses**, sometimes including extra options.\n",
    "\n",
    "  `df.head()`       → shows the first 5 rows  \n",
    "  `df.describe()`   → summary statistics  \n",
    "\n",
    "Lets start by exploring the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dimensions of the DataFrame (rows, columns)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the names of the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a concise summary of the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics\n",
    "\n",
    "Pandas provides powerful tools for quickly summarising your data. The `describe()` method is a great way to get an overview of the numerical columns in your DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary statistics for numerical columns\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Summary Statistics\n",
    "\n",
    "The `describe()` output gives us a great first look at our numerical data. Let's break down what each row means:\n",
    "\n",
    "- **count:** The number of non-missing (non-NaN) values. Notice that `age` and `salary` have fewer than 20, which tells us there are missing values.\n",
    "- **mean:** The average value. For `age`, the mean is very high because of the outlier (999). This shows how the mean is sensitive to extreme values.\n",
    "- **std (Standard Deviation):** A measure of how spread out the data is. A high standard deviation (like in `salary`) means the values are widely distributed.\n",
    "- **min:** The smallest value in the column.\n",
    "- **25% (1st Quartile):** 25% of the data points are smaller than this value.\n",
    "- **50% (Median):** The middle value of the dataset. 50% of the data is below this value, and 50% is above. The median is often a better measure of central tendency than the mean when there are outliers.\n",
    "- **75% (3rd Quartile):** 75% of the data points are smaller than this value.\n",
    "- **max:** The largest value in the column. The `max` for `age` (999) is clearly a data quality issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Data\n",
    "\n",
    "Real-world data is often messy and contains missing values. Pandas represents missing values as `NaN` (Not a Number). It's crucial to identify and handle these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Missing Values\n",
    "\n",
    "We can use the `.isnull()` method, which returns a DataFrame of boolean values, followed by `.sum()` to count the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the `describe()` output, `age` and `salary` each have one missing value. `performance_score` also has a missing value.\n",
    "\n",
    "### How Pandas Handles NaNs in Calculations\n",
    "\n",
    "By default, pandas **ignores** `NaN` values when performing calculations like `mean()`, `sum()`, etc. This is why the `count` in `describe()` was lower for columns with missing data—pandas simply doesn't include them in the calculation, which is usually the desired behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Columns\n",
    "\n",
    "Let's dive deeper into individual columns to understand their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Counts\n",
    "\n",
    "The `.value_counts()` method is great for categorical data. It shows how many times each unique value appears in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the value counts for the 'department' column\n",
    "df['department'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique Values\n",
    "\n",
    "You can get the number of unique values with `.nunique()` or a list of the unique values with `.unique()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_depts = df['department'].nunique()\n",
    "\n",
    "print(f\"Number of unique departments: {unique_depts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Type Inference and Casting\n",
    "\n",
    "Pandas is smart and tries to infer the data type of each column when you load data. However, sometimes it gets it wrong, especially if there are mixed types. For example, our `performance_score` column contains numbers and the string 'N/A', so pandas has classified the whole column as an `object` (which usually means string).\n",
    "\n",
    "We can fix this by first replacing 'N/A' with a proper missing value (`np.nan`) and then casting the column to a numeric type using `.astype()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check the data types\n",
    "print(\"Original data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Create dictionary to of values to replace in performance score column\n",
    "score_cleaning = {'TBC': np.nan,\n",
    "                 'Full score': 5}\n",
    "\n",
    "# Replace 'TBC' with NaN and 'Full Score' with 5\n",
    "df['performance_score'] = df['performance_score'].replace(score_cleaning)\n",
    "\n",
    "# Cast column to float data type\n",
    "df['performance_score'] = df['performance_score'].astype(float)\n",
    "\n",
    "print(\"New data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting and Indexing\n",
    "\n",
    "Often, you only want to look at a specific part of your DataFrame. Pandas provides powerful tools for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns\n",
    "\n",
    "You can select a single column using square brackets `[]`, which returns a pandas Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the 'name' column\n",
    "df['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting with `.loc` (Label-Based)\n",
    "\n",
    "`.loc` is used for selecting data by row and column **labels**. The syntax is `df.loc[row_labels, column_labels]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select row with index 3 and the 'name' and 'salary' columns\n",
    "df.loc[3, ['name', 'salary']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting with `.iloc` (Position-Based)\n",
    "\n",
    "`.iloc` is used for selecting data by row and column **integer positions**. The syntax is `df.iloc[row_positions, column_positions]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the first 3 rows and the first 2 columns\n",
    "df.iloc[0:3, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the DataFrame\n",
    "\n",
    "Let's see how we can add, remove, and change data in our DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping a Column\n",
    "\n",
    "You can remove a column using the `.drop()` method. You need to specify `axis=1` to indicate you're dropping a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the 'employee_id' column\n",
    "df_dropped = df.drop('employee_id', axis=1)\n",
    "df_dropped.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Column\n",
    "\n",
    "You can create a new column by simply assigning it a value. Let's create a 'age_when_joined' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new calculated column\n",
    "df['age_when_joined'] = df['age'] - df['years_at_company']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing Values\n",
    "\n",
    "The `.replace()` method is useful for updating specific values. Let's say we want to give our departments more descriptive names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace department names\n",
    "df['department'] = df['department'].replace({'HR': 'Human Resources', 'Engineering': 'Tech'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Data Quality\n",
    "\n",
    "Filtering is one of the most common tasks in data analysis. You can create a boolean condition and use it to select rows from your DataFrame.\n",
    "\n",
    "This is also a great way to identify rows with data quality issues. For example, we know that an age of 999 is impossible. Let's find that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rows where age is greater than 100\n",
    "df[df['age'] > 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the problematic row and decide how to handle it (e.g., correct it if we know the right age, or remove it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important: DataFrames are In-Memory Only\n",
    "\n",
    "It's crucial to understand that when you load data into a pandas DataFrame, it exists only in your computer's memory (RAM). This means:\n",
    "\n",
    "- **Changes are temporary:** Any modifications you make (cleaning data, adding columns, fixing typos) only exist while your program is running.\n",
    "- **Original file unchanged:** The original CSV file remains exactly as it was when you loaded it.\n",
    "- **Data lost when program ends:** If you close your notebook or restart your Python session, all your changes disappear.\n",
    "\n",
    "### When to Save Your Work\n",
    "\n",
    "You should save your DataFrame to a new file when:\n",
    "- You've cleaned the data and want to preserve those improvements\n",
    "- You've added new calculated columns that took time to create\n",
    "- You want to share the cleaned dataset with others\n",
    "- You're finished with your analysis and want to keep the final version\n",
    "\n",
    "Let's see how to save our cleaned employee dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df.to_csv('employee_data_cleaned.csv', index=False)\n",
    "\n",
    "# You can also save just specific columns if needed\n",
    "df[['name', 'department', 'salary']].to_csv('employee_summary.csv', index=False)\n",
    "\n",
    "# Note: index=False prevents pandas from saving the row numbers as a separate column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Thoughts\n",
    "\n",
    "You've covered a lot of ground! This notebook provides a solid foundation for the most common pandas operations. The topics you requested are comprehensive for an introduction. The next logical steps, which you could introduce in a future workshop, would be:\n",
    "\n",
    "- **Grouping and Aggregation:** Using `.groupby()` to perform calculations on specific groups within the data (e.g., finding the average salary per department).\n",
    "- **Merging and Joining:** Combining multiple DataFrames, similar to SQL joins.\n",
    "- **More advanced plotting:** Using libraries like Matplotlib or Seaborn to create more complex visualisations from the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
